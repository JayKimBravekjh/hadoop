# First create some input data and put it in the HDFS. Make two or more files named urlX where X is a number. 
# Each file should contain exactly one URL.  For example, here create two files:

# echo "http://www.cs.brandeis.edu" >url1
# echo "http://www.nytimes.com" >url2


## This creates these two files:

# url1 
## http://www.cs.brandeis.edu

# url2 
## http://www.nytimes.com 

# bin/hadoop dfs -mkdir urls 
# bin/hadoop dfs -put url1 urls/
# bin/hadoop dfs -put url2 urls/

## may be able to come up with a more efficient way to put many such url files into HDFS.

# run the command: 
# bin/hadoop jar contrib/hadoop-0.15.2-streaming.jar \
#  -mapper  $HOME/proj/hadoop/multifetch.py          \
#  -reducer $HOME/proj/hadoop/reducer.py             \
#  -input   urls/*                                   \
#  -output  titles                                   \

## output as below 
# null=@@@userJobConfProps_.get(stream.shipped.hadoopstreaming
# packageJobJar: [/tmp/hadoop-ross/hadoop-unjar5997/] []
#  /tmp/streamjob5998.jar tmpDir=null
# 08/01/11 10:34:43 INFO mapred.FileInputFormat: Total input paths to process : 5
# 08/01/11 10:34:44 INFO streaming.StreamJob: getLocalDirs():
#  [/tmp/hadoop-ross/mapred/local]
# 08/01/11 10:34:44 INFO streaming.StreamJob: Running job: job_200801111003_0003
# 08/01/11 10:34:44 INFO streaming.StreamJob: To kill this job, run:
# 08/01/11 10:34:44 INFO streaming.StreamJob:
#  /home/ross/cs147a/hadoop/hadoop-0.15.2/bin/../bin/hadoop job
#  -Dmapred.job.tracker=localhost:9001 -kill job_200801111003_0003
# 08/01/11 10:34:44 INFO streaming.StreamJob:
#  Tracking URL: http://localhost:50030/jobdetails.jsp?jobid=job_200801111003_0003
# 08/01/11 10:34:45 INFO streaming.StreamJob:  map 0%  reduce 0%
# 08/01/11 10:34:52 INFO streaming.StreamJob:  map 40%  reduce 0%
# 08/01/11 10:34:53 INFO streaming.StreamJob:  map 80%  reduce 0%
# 08/01/11 10:34:54 INFO streaming.StreamJob:  map 100%  reduce 0%
# 08/01/11 10:35:03 INFO streaming.StreamJob:  map 100%  reduce 100%
# 08/01/11 10:35:03 INFO streaming.StreamJob: Job complete: job_200801111003_0003
# 08/01/11 10:35:03 INFO streaming.StreamJob: Output: titles


# can see the result as below. 
# bin/hadoop dfs -cat titles/part-00000
# http://www.cs.brandeis.edu  Michtom School of Computer Science
# http://www.nytimes.com      The New York Times - Blah blah ...
